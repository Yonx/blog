---
title: PDNS系统设计实现总结（on going）
date: 2017-03-13 21:45:40
tags:
 - PDNS 
 - DNS 
---

{% centerquote %}
不久前，我遇上一个人，送给我一坛酒
她说那叫“醉生梦死”
喝过之后，可以忘掉做过的任何事。
我很奇怪，为什么会有这样的酒。
她说人最大的烦恼，就是记性太好
如果什么都可以忘掉，以后的每一天将会是一个新的开始
那你说这有多开心。
      --- ** 黄药师 **  * 《东邪西毒》 *
{% endcenterquote %}
> "来，当3个月程序员试试看"                  --- ** 我 ** 


---

# 数据采集点的区别

数据分析的前提是要懂数据，懂数据的前提就是要知道数据从哪来，是什么样子的，从而可以知道对于得到的数据，那些能做，哪些不能做。不同的采集点，采集到的数据不一样，量有大小的区别，覆盖范围有区别，可以提取出来的特征不一样，因此对于既定的分析目标，可能一个采集点的数据可以轻而易举的完成，而另外一个采集点可能做起来会非常费力，甚或天然的就无法做到。

![](/2017/03/13/pdns-process-notes/pdns.frame.png)
> 由于懒，我直接抠我在FloCon2017上的talk的PPT的一页来说明。

上图是一个最简单的DNS请求的全路径。一个用户在运营商提供的一个子网内，发起的DNS请求通过运营商的边界路由，到一个OpenResolver/RecursiveServer，OpenResolver/RecursiveServer 负责完整的递归查询，将最终的IP返回给用户


## open resolver 之上
图中的点【1】处。

这里的数据是DNS服务商的递归查询数据。理论上，只有当 

* 一个查询的域名之前没有查询过 
* 一个已经缓存的域名结果TTL已经过期

两种情况下，才会有递归数据产生。

域名的请求永远是大头长尾的数据形态，大部分的查询都会落在缓存中，有效的TTL时间范围内，一般来说，这里的数据量比点【2】处的客户端查询要小2个数量级。

加之一般的DNS服务商都会有基本的数据过滤，不合法的请求，错误的数据包，基本都可以很轻松的清洗掉，所以这里的数据也会比较干净。

数据量小，而且干净，拥有所有的递归过程的数据，因此这里的数据最适合用于构建一个PDNS系统，用以记录历史上domain-ip的映射关系及其变化。

## open resolver 之下
图中的点【2】处。

这里的数据包括所有的点【1】的数据，不过数据量至少至少上升了2个数量级。

此外，在这里我们看得到客户端的数据，我们可以知道一个client ip在什么时间请求了什么域名。一个client ip频繁的请求比如cpsc.gov ANY，这极有可能是反射放大。一个client ip请求同一个SLD的不同的子域名，这又分至少两种情况：子域名如果构成比较规律，比如一个单词，那可能是子域名暴力破解；子域名如果构成是杂乱的随机字符串，那可能是RSD攻击。

而且，我们在此还可以知道query数据包中的src port（sport），transaction id（tid）数据。真实的DNS请求都是伴随着随机的sport/tid，因此，在一段时间内，针对同一个domain的所有query，或者一个client发出去的针对任意domain的query，其sport/tid的统计肯定是离散的，当统计显示针对某domain／某client的sport/tid是聚集的时候，我们就有相当大的把握断定这部分数据是伪造的query。

## 路由器边界
图中的点【3】处。

乍看起来，点【3】的数据等于把所有点【2】的数据都汇聚到一起。现实的问题在于：1）点【2】的数据都属于各大DNS厂商，这部分数据不可能完全汇总分享 2）正常的用户往往都会使用一个DNS服务器，但是和DNS相关的攻击流量都会和很多的open resolver相关 3）有很多DNS流量和open resolver无关，在点【2】也看不到，因此点【3】是非常必要的。

举例来说，RSD攻击一般来说会通过多个open resolver来打，但是也可能伪造流量直接请求到authoritative server，如果是后者，那在点【2】的位置就完全看不到，但是在点【3】可以看到。

再者，点【3】是client-focused，比如反射放大攻击，一次攻击可能动用上万个open resolver，在点【2】的单个open resolver处很可能被忽略掉，但是点【3】看到的是一个client ip接受来自上万个open resolver的响应，想故意漏掉都比较难吧。


甚或，这里我们可以看到query without response 有去无回数据, response without query无中生有数据，看到一个DNS服务器将任意domain的query响应为一个固定的IP等等，具体有什么用，think～

## 其他
上述三个数据采集点是做DNS数据分析一般的，常规的，最有效的数据采集位置，此外前后两端的数据采集点也要注意。
### authority server 边界

麻烦各个NS管理员，如果闲了，看看自己的DNS服务器处理的请求都是什么，有没有开泛解析，有没有配置错误将一个权威服务器开启了递归查询功能。我们的数据表明，常被利用的DNS反射节点中，至少2%是开了递归查询功能的权威服务器，无意间就给反射放大攻击添柴助力。

### 客户端网卡

说一个场景，用户电脑，没人操作的时候，恶意软件可没闲着，各种可疑的黑网站，DGA这时候都会突兀的出现。

### 不可说

实际上，任何可以获取DNS解析记录的地方，其数据可能都有独到可用之处。尤其考虑到数据获取的场景context，简单来说，场景越黑，数据越黑。你有黑场景的数据可分享么？如果有，请联系我～

---


# 接入点处理

当前我们的数据在白天忙时平均有700w records/s，record指得是query-response pairing之后提取出来的数据记录。多个数据节点数据并不是平均分配的，最大的点超过150w/s，接入的千兆网卡是打满的状态。针对这么大的数据量，系统架构，或者说数据流设计，都要依赖一个高效稳固的接入和足够灵活的数据分发方式，所以接入点的处理单独拎出来说明一下。

* sensor：负责原始DNS流量的抓取，解析，配对过程，形成最终的record，然后以hash(client ip/24)为key将数据publish出来
* hasher：接受从sensor的record数据，然后以hash(SLD)的形式将数据publish出来

{% note default %}
考虑到用户隐私，client ip可以做混淆
{% endnote %}

## 应对超大量数据

* 数据水平切分，client／domain两大维度，后续详细分析
* 传输使用Zmq，pub/sub模式，单ctx足够
* 数据格式为protobuf。另：注意所有字段都为optional，不明白原因的去google
* 批量合并压缩数据，zlib的Z_BEST_SPEED模式下压缩率为30%左右
* 无锁队列，zmq的push/pull (inproc://addr)。注意，一定是消费者同质的时候才可以。消费者不同质，老老实实上lock-queue，否则一个慢消费者会拖死整个队列
* log要异步多线程flush
* 打点统计尽可能避免锁，可以使用__sync_XXX系列函数，也可以考虑thread::local单独打点，合并dump

## 数据分割

**<u>这一点尤为关键</u>**，接入的数据量巨大，不可能根据不同的需求重复传输多次，只有水平切分做好，后续的处理过程才能非常方便的扩展。

我的架构里，sensor和hasher作为公共的数据获取接口，其中sensor是以hash(client ip)为key的获取接口，hasher是以hash(SLD)为key的获取接口。

根据需求，如果后续的分析过程是以domain为核心的，那就从hasher来获取，所有*.test.domain都会被分发到一个同一个key下。如果想看某个client ip的情况，那就从sensor直接获取，那同一个client ip的访问行为会集中发布在同一个key上。

提取SLD的过程，不要简单的从后向前数点，com.cn等多级的TLD和com等单级的TLD判断起来会比较麻烦费力。先将所有TLD数据load成为一个trie tree，来的每一条数据，将FQDN从后向前遍历到最深，然后接着遍历到结尾或者下一个.的位置即可，考虑到SLD的长度基本都会在10个字符以内，这样的算法可以认为是O(1)的。

## disposable domain

## 去重

## “阶梯”采样

---

# 基本架构／数据流

## PDNS system

## cross-access system

## real-time data query system

## real-time analysis system

---

# 实时分析处理

## 归一化及数据预处理

## 域名请求量统计

## spike 判定

## domain profile

## client profile

## DNS server profile

## 特征选择

## 随机程度的度量：熵？

---

# 其他

## 日志系统

## 打点统计

## 分发缓存






